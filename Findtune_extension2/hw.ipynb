{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78facf24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import gc\n",
    "import argparse\n",
    "import json\n",
    "import random\n",
    "import math\n",
    "import random\n",
    "from functools import reduce\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "from sklearn.model_selection import train_test_split, ShuffleSplit, StratifiedShuffleSplit, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, precision_recall_fscore_support, classification_report\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam, SGD, AdamW\n",
    "from torch.nn import functional as F\n",
    "from torch.optim.lr_scheduler import StepLR, CosineAnnealingWarmRestarts, CyclicLR\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import torch.distributed as dist\n",
    "\n",
    "from performer_pytorch import PerformerLM\n",
    "import scanpy as sc\n",
    "import anndata as ad\n",
    "from utils import *\n",
    "import pickle as pkl\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--local_rank\", type=int, default=-1, help='Local process rank.')\n",
    "parser.add_argument(\"--bin_num\", type=int, default=5, help='Number of bins.')\n",
    "parser.add_argument(\"--gene_num\", type=int, default=16906, help='Number of genes.')\n",
    "parser.add_argument(\"--epoch\", type=int, default=100, help='Number of epochs.')\n",
    "parser.add_argument(\"--seed\", type=int, default=2021, help='Random seed.')\n",
    "parser.add_argument(\"--batch_size\", type=int, default=3, help='Number of batch size.')\n",
    "parser.add_argument(\"--grad_acc\", type=int, default=60, help='Number of gradient accumulation.')\n",
    "parser.add_argument(\"--valid_every\", type=int, default=1, help='Number of training epochs between twice validation.')\n",
    "parser.add_argument(\"--pos_embed\", type=bool, default=True, help='Using Gene2vec encoding or not.')\n",
    "parser.add_argument(\"--data_path\", type=str, default='./data/Zheng68K.h5ad', help='Path of data for finetune.')\n",
    "parser.add_argument(\"--model_path\", type=str, default='./panglao_pretrained.pth', help='Path of pretrained model.')\n",
    "parser.add_argument(\"--ckpt_dir\", type=str, default='./ckpts/', help='Directory of checkpoint to save.')\n",
    "parser.add_argument(\"--model_name\", type=str, default='finetune', help='Finetuned model name.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c87cd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initial learning rate\n",
    "parser.add_argument(\"--learning_rate\", type=float, default=1e-4, help='Initial learning rate.')\n",
    "\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "#shared cross layer parameters\n",
    "class Identity(torch.nn.Module):\n",
    "    def __init__(self, dropout = 0., h_dim = 100, out_dim = 10):\n",
    "        super(Identity, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 1, (1, 200))\n",
    "        self.act = nn.ReLU()\n",
    "        # Shared fully connected layer\n",
    "        self.shared_fc = nn.Linear(in_features=SEQ_LEN, out_features=512, bias=True)\n",
    "        \n",
    "        self.act1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        \n",
    "        # Note: fc2 is removed and replaced by shared_fc\n",
    "        \n",
    "        self.act2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.fc3 = nn.Linear(in_features=512, out_features=out_dim, bias=True)  # Adjusted the input features\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x[:, None, :, :]\n",
    "        x = self.conv1(x)\n",
    "        x = self.act(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        \n",
    "        x = self.shared_fc(x)  # First use of shared_fc\n",
    "        x = self.act1(x)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        x = self.shared_fc(x)  # Second use of shared_fc\n",
    "        x = self.act2(x)\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad1a41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parser.parse_args()\n",
    "rank = int(os.environ[\"RANK\"])\n",
    "local_rank = args.local_rank\n",
    "is_master = local_rank == 0\n",
    "\n",
    "SEED = args.seed\n",
    "EPOCHS = args.epoch\n",
    "BATCH_SIZE = args.batch_size\n",
    "GRADIENT_ACCUMULATION = args.grad_acc\n",
    "LEARNING_RATE = args.learning_rate\n",
    "SEQ_LEN = args.gene_num + 1\n",
    "VALIDATE_EVERY = args.valid_every\n",
    "\n",
    "PATIENCE = 10\n",
    "UNASSIGN_THRES = 0.0\n",
    "\n",
    "CLASS = args.bin_num + 2\n",
    "POS_EMBED_USING = args.pos_embed\n",
    "\n",
    "model_name = args.model_name\n",
    "ckpt_dir = args.ckpt_dir\n",
    "\n",
    "dist.init_process_group(backend='nccl')\n",
    "torch.cuda.set_device(local_rank)\n",
    "device = torch.device(\"cuda\", local_rank)\n",
    "world_size = torch.distributed.get_world_size()\n",
    "\n",
    "seed_all(SEED + torch.distributed.get_rank())\n",
    "\n",
    "\n",
    "class SCDataset(Dataset):\n",
    "    def __init__(self, data, label):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        self.label = label\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        rand_start = random.randint(0, self.data.shape[0]-1)\n",
    "        full_seq = self.data[rand_start].toarray()[0]\n",
    "        full_seq[full_seq > (CLASS - 2)] = CLASS - 2\n",
    "        full_seq = torch.from_numpy(full_seq).long()\n",
    "        full_seq = torch.cat((full_seq, torch.tensor([0]))).to(device)\n",
    "        seq_label = self.label[rand_start]\n",
    "        return full_seq, seq_label\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "\n",
    "\n",
    "data = sc.read_h5ad(args.data_path)\n",
    "label_dict, label = np.unique(np.array(data.obs['celltype']), return_inverse=True)  # Convert strings categorical to integrate categorical, and label_dict[label] can be restored\n",
    "#store the label dict and label for prediction\n",
    "with open('label_dict', 'wb') as fp:\n",
    "    pkl.dump(label_dict, fp)\n",
    "with open('label', 'wb') as fp:\n",
    "    pkl.dump(label, fp)\n",
    "class_num = np.unique(label, return_counts=True)[1].tolist()\n",
    "class_weight = torch.tensor([(1 - (x / sum(class_num))) ** 2 for x in class_num])\n",
    "label = torch.from_numpy(label)\n",
    "data = data.X\n",
    "\n",
    "acc = []\n",
    "f1 = []\n",
    "f1w = []\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "pred_list = pd.Series(['un'] * data.shape[0])\n",
    "\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=SEED)\n",
    "for index_train, index_val in sss.split(data, label):\n",
    "    data_train, label_train = data[index_train], label[index_train]\n",
    "    data_val, label_val = data[index_val], label[index_val]\n",
    "    train_dataset = SCDataset(data_train, label_train)\n",
    "    val_dataset = SCDataset(data_val, label_val)\n",
    "\n",
    "train_sampler = DistributedSampler(train_dataset)\n",
    "val_sampler = DistributedSampler(val_dataset)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=train_sampler)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, sampler=val_sampler)\n",
    "\n",
    "model = PerformerLM(\n",
    "    num_tokens = CLASS,\n",
    "    dim = 200,\n",
    "    depth = 6,\n",
    "    max_seq_len = SEQ_LEN,\n",
    "    heads = 10,\n",
    "    local_attn_heads = 0,\n",
    "    g2v_position_emb = POS_EMBED_USING\n",
    ")\n",
    "\n",
    "path = args.model_path\n",
    "ckpt = torch.load(path)\n",
    "model.load_state_dict(ckpt['model_state_dict'])\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model.norm.parameters():\n",
    "    param.requires_grad = True\n",
    "for param in model.performer.net.layers[-2].parameters():\n",
    "    param.requires_grad = True\n",
    "model.to_out = Identity(dropout=0., h_dim=128, out_dim=label_dict.shape[0])\n",
    "model = model.to(device)\n",
    "model = DDP(model, device_ids=[local_rank], output_device=local_rank)\n",
    "\n",
    "# optimizer\n",
    "optimizer = Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss(weight=None).to(local_rank)\n",
    "\n",
    "dist.barrier()\n",
    "trigger_times = 0\n",
    "max_acc = 0.0\n",
    "for i in range(1, EPOCHS+1):\n",
    "    train_loader.sampler.set_epoch(i)\n",
    "    model.train()\n",
    "    dist.barrier()\n",
    "    running_loss = 0.0\n",
    "    cum_acc = 0.0\n",
    "    for index, (data, labels) in enumerate(train_loader):\n",
    "        index += 1\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "        if index % GRADIENT_ACCUMULATION != 0:\n",
    "            with model.no_sync():\n",
    "                logits = model(data)\n",
    "                loss = loss_fn(logits, labels)\n",
    "                #total_loss.backward()\n",
    "                loss.backward()\n",
    "        if index % GRADIENT_ACCUMULATION == 0:\n",
    "            logits = model(data)\n",
    "            loss = loss_fn(logits, labels)\n",
    "            #total_loss.backward()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), int(1e6))\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        running_loss += loss.item()\n",
    "        softmax = nn.Softmax(dim=-1)\n",
    "        final = softmax(logits)\n",
    "        final = final.argmax(dim=-1)\n",
    "        pred_num = labels.size(0)\n",
    "        correct_num = torch.eq(final, labels).sum(dim=-1)\n",
    "        cum_acc += torch.true_divide(correct_num, pred_num).mean().item()\n",
    "    epoch_loss = running_loss / index\n",
    "    epoch_acc = 100 * cum_acc / index\n",
    "    epoch_loss = get_reduced(epoch_loss, local_rank, 0, world_size)\n",
    "    epoch_acc = get_reduced(epoch_acc, local_rank, 0, world_size)\n",
    "    if is_master:\n",
    "        print(f'    ==  Epoch: {i} | Training Loss: {epoch_loss:.6f} | Accuracy: {epoch_acc:6.4f}%  ==')\n",
    "    dist.barrier()\n",
    "    scheduler.step()\n",
    "\n",
    "    if i % VALIDATE_EVERY == 0:\n",
    "        model.eval()\n",
    "        dist.barrier()\n",
    "        running_loss = 0.0\n",
    "        predictions = []\n",
    "        truths = []\n",
    "        with torch.no_grad():\n",
    "            for index, (data_v, labels_v) in enumerate(val_loader):\n",
    "                index += 1\n",
    "                data_v, labels_v = data_v.to(device), labels_v.to(device)\n",
    "                logits = model(data_v)\n",
    "                loss = loss_fn(logits, labels_v)\n",
    "                running_loss += loss.item()\n",
    "                softmax = nn.Softmax(dim=-1)\n",
    "                final_prob = softmax(logits)\n",
    "                final = final_prob.argmax(dim=-1)\n",
    "                final[np.amax(np.array(final_prob.cpu()), axis=-1) < UNASSIGN_THRES] = -1\n",
    "                predictions.append(final)\n",
    "                truths.append(labels_v)\n",
    "            del data_v, labels_v, logits, final_prob, final\n",
    "            # gather\n",
    "            predictions = distributed_concat(torch.cat(predictions, dim=0), len(val_sampler.dataset), world_size)\n",
    "            truths = distributed_concat(torch.cat(truths, dim=0), len(val_sampler.dataset), world_size)\n",
    "            no_drop = predictions != -1\n",
    "            predictions = np.array((predictions[no_drop]).cpu())\n",
    "            truths = np.array((truths[no_drop]).cpu())\n",
    "            cur_acc = accuracy_score(truths, predictions)\n",
    "            f1 = f1_score(truths, predictions, average='macro')\n",
    "            val_loss = running_loss / index\n",
    "            val_loss = get_reduced(val_loss, local_rank, 0, world_size)\n",
    "            if is_master:\n",
    "                print(f'    ==  Epoch: {i} | Validation Loss: {val_loss:.6f} | F1 Score: {f1:.6f}  ==')\n",
    "                print(confusion_matrix(truths, predictions))\n",
    "                print(classification_report(truths, predictions, target_names=label_dict.tolist(), digits=4))\n",
    "            if cur_acc > max_acc:\n",
    "                max_acc = cur_acc\n",
    "                trigger_times = 0\n",
    "                save_best_ckpt(i, model, optimizer, scheduler, val_loss, model_name, ckpt_dir)\n",
    "            else:\n",
    "                trigger_times += 1\n",
    "                if trigger_times > PATIENCE:\n",
    "                    break\n",
    "    del predictions, truths\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59114fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# knowledge distillation\n",
    "class SimpleStudentModel(nn.Module):\n",
    "    def __init__(self, dropout = 0, h_dim = 100, out_dim = 10):\n",
    "        super(SimpleStudentModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 1, (1, 200))\n",
    "        self.act = nn.ReLU()\n",
    "        # Shared fully connected layer\n",
    "        self.fc1 = nn.Linear(in_features=SEQ_LEN, out_features=100, bias=True)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.fc2 = nn.Linear(in_features=100, out_features=56, bias=True)\n",
    "        self.act2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.fc2 = nn.Linear(in_features=56, out_features=out_dim, bias=True)  # Adjusted the input features\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x[:, None, :, :]\n",
    "        x = self.conv1(x)\n",
    "        x = self.act(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        \n",
    "        x = self.fc1(x)  \n",
    "        x = self.act1(x)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        x = self.fc2(x) \n",
    "        x = self.act2(x)\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "student_model = SimpleStudentModel(dropout = 0, h_dim = 100, out_dim = 10)\n",
    "\n",
    "def softmax_with_temperature(logits, temperature):\n",
    "    return torch.nn.functional.softmax(logits / temperature, dim=-1)\n",
    "\n",
    "# Hyperparameters\n",
    "temperature = 5  # Temperature used to soften the probabilities\n",
    "alpha = 0.5      # Weight for the distillation loss relative to the true label loss\n",
    "student_optimizer = torch.optim.Adam(student_model.parameters(), lr=1e-4)\n",
    "\n",
    "for data, labels in train_loader:\n",
    "    # Forward pass of the teacher with input data\n",
    "    teacher_outputs = teacher_model(data)\n",
    "    teacher_soft_outputs = softmax_with_temperature(teacher_outputs, temperature)\n",
    "\n",
    "    # Forward pass of the student\n",
    "    student_outputs = student_model(data)\n",
    "\n",
    "    # Calculate loss\n",
    "    student_loss = nn.KLDivLoss()(F.log_softmax(student_outputs/temperature, dim=-1),\n",
    "                                  F.softmax(teacher_soft_outputs/temperature, dim=-1)) * (alpha * temperature * temperature) + \\\n",
    "                   F.cross_entropy(student_outputs, labels) * (1. - alpha)\n",
    "\n",
    "    # Backward pass and optimize\n",
    "    student_optimizer.zero_grad()\n",
    "    student_loss.backward()\n",
    "    student_optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb85c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-Tuning the Scheduler Parameters\n",
    "from itertools import product\n",
    "\n",
    "scheduler = CosineAnnealingWarmupRestarts(\n",
    "    optimizer,\n",
    "    first_cycle_steps=15,  # Number of steps for the first cycle\n",
    "    cycle_mult=2,         # Scaling factor for subsequent cycles\n",
    "    max_lr=LEARNING_RATE, # Maximum learning rate\n",
    "    min_lr=1e-6,          # Minimum learning rate\n",
    "    warmup_steps=5,       # Number of warmup steps\n",
    "    gamma=0.9             # Factor for reducing the max_lr after each cycle\n",
    ")\n",
    "\n",
    "# Define the ranges for each hyperparameter\n",
    "first_cycle_steps_range = [5, 10, 15, 20]\n",
    "cycle_mult_range = [1, 2, 3]\n",
    "max_lr_range = [0.001, 0.0001, 0.00001]\n",
    "warmup_steps_range = [3, 5, 7, 9]\n",
    "\n",
    "# Placeholder for the best score and corresponding hyperparameters\n",
    "best_score = float('-inf')\n",
    "best_params = {}\n",
    "\n",
    "# Iterate over all combinations\n",
    "for first_cycle_steps, cycle_mult, max_lr, min_lr, warmup_steps, gamma in product(\n",
    "    first_cycle_steps_range, cycle_mult_range, max_lr_range,\n",
    "    min_lr_range, warmup_steps_range, gamma_range):\n",
    "\n",
    "    # Initialize the scheduler with the current set of hyperparameters\n",
    "    scheduler = CosineAnnealingWarmupRestarts(\n",
    "        optimizer,\n",
    "        first_cycle_steps=first_cycle_steps,\n",
    "        cycle_mult=cycle_mult,\n",
    "        max_lr=max_lr,\n",
    "        min_lr=min_lr,\n",
    "        warmup_steps=warmup_steps,\n",
    "        gamma=gamma\n",
    "    )\n",
    "    \n",
    "    if current_score > best_score:\n",
    "        best_score = current_score\n",
    "        best_params = {\n",
    "            'first_cycle_steps': first_cycle_steps,\n",
    "            'cycle_mult': cycle_mult,\n",
    "            'max_lr': max_lr,\n",
    "            'min_lr': min_lr,\n",
    "            'warmup_steps': warmup_steps,\n",
    "            'gamma': gamma\n",
    "        }\n",
    "\n",
    "# Print the best parameters\n",
    "print(\"Best score:\", best_score)\n",
    "print(\"Best hyperparameters:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ca9505",
   "metadata": {},
   "outputs": [],
   "source": [
    "#triple loss\n",
    "lm_loss_fn = nn.CrossEntropyLoss()\n",
    "lm_loss = loss_fn \n",
    "\n",
    "def softmax_with_temperature(logits, temperature):\n",
    "    return F.softmax(logits / temperature, dim=-1)\n",
    "\n",
    "kl_div_loss_fn = nn.KLDivLoss(reduction='batchmean')\n",
    "distillation_loss = kl_div_loss_fn(\n",
    "    F.log_softmax(student_logits / temperature, dim=-1),\n",
    "    softmax_with_temperature(teacher_logits, temperature)\n",
    ")\n",
    "\n",
    "cosine_loss_fn = nn.CosineEmbeddingLoss()\n",
    "\n",
    "# `embeddings1` and `embeddings2` are the embeddings you want to pull closer\n",
    "# `target` is a tensor of -1s or 1s indicating whether the embeddings should be\n",
    "# pulled closer (1) or pushed apart (-1)\n",
    "cosine_loss = cosine_loss_fn(embeddings1, embeddings2, target)\n",
    "\n",
    "# Weights for each component of the loss\n",
    "alpha = 0.4\n",
    "beta = 0.4\n",
    "gamma = 0.2\n",
    "\n",
    "total_loss = alpha * lm_loss + beta * distillation_loss + gamma * cosine_loss\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
